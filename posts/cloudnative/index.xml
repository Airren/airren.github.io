<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ByteGopher</title>
    <link>https://airren.github.io/posts/cloudnative/</link>
    <description>Recent content on ByteGopher</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 29 Dec 2018 11:02:05 +0800</lastBuildDate><atom:link href="https://airren.github.io/posts/cloudnative/index.xml" rel="self" type="application/rss+xml" /><item>
      <title></title>
      <link>https://airren.github.io/posts/cloudnative/Istio/1-Istio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Istio/1-Istio/</guid>
      <description>Service Mesh 浅析
Istio的自我救赎
https://github.com/cloudnativebooks/cloud-native-istio/tree/master/chapter-files/security
云原生架构的三驾马车
Kubenetes， Service Mesh , Serverless
Servcie Mesh 超时重试
16年概念提出。
技术选型？
ServiceMesh 的相关概念
Service Mesh的起源
微服务架构的特性
围绕业务构建团队 康威定律-&amp;gt; 团队结构决定了产品结构
去中心化的数据管理
团队层面： 内聚，独立业务开发，没有依赖
产品层面： 服务彼此独立，独立部署，没有依赖
访问量决定部署实例的数量
银弹理论-&amp;gt; 人月神话 没有任何一种技术可以完美的解决软件开发中的问题。
空间换时间 or 时间换空间
微服务架构带来的缺点
服务间网络通信问题
分布式计算的8个谬论
网络是可靠的 网络延时是0 很难会把网络相关的需求考虑到我们的设计中。分布式系统中，网络问题是一个重要问题。
如何管理和控制网络通信
辅助注册、发现
路由，流量转移
弹性能力 熔断超时重试
安全
可观测性
Patten： Service Mesh
阶段一： 控制逻辑与业务逻辑耦合
阶段二： 公共库：流控，重试 （人力，时间学习，语言蚌绑定，平台相关，代码侵入）
阶段三：代理模式， 功能简陋Nginx
阶段四： Sidecar 模式 2013-2105
阶段五： Service Mesh 2016-2017
​ Service MeshV2 2018</description>
    </item>
    
    <item>
      <title></title>
      <link>https://airren.github.io/posts/cloudnative/Kubernates/k8s-Istio-Envoy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Kubernates/k8s-Istio-Envoy/</guid>
      <description>What is Service Mesh(服务网格) https://jimmysong.io/blog/why-do-you-need-istio-when-you-already-have-kubernetes/
UPF
2 次链接
1 控制面 ip register step 1-4
hub 公有网络直连接， 给edge pod分配全局IP，cluster api server CRD controller 监视CRs &amp;ndash;&amp;gt; kube cofig
2 数据面
hub-device connection
hub 桥接
IPSec configuration -&amp;gt; 2 次分发
SNAT-&amp;gt; DNAT IP 地址可能重合
测试环境部署 4 cluster
物理机 4 nuc
10.239.241.255 &amp;ndash; hub
switch 交换机
kubectl get iphost
kubectl describe iphost hubdevice1
kubectl get
spce:
pubkry
conn_type
proposal 加解密
mark route base VPN</description>
    </item>
    
    <item>
      <title></title>
      <link>https://airren.github.io/posts/cloudnative/Linux/curl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Linux/curl/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://airren.github.io/posts/cloudnative/Linux/iptables/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Linux/iptables/</guid>
      <description>TCP 的有限状态机
CLOSED CLOSED
SYN_SENT LISTENING
​ SYN_RECEIVED
ESTABLISHED ESTABLISHED
netfilter: Frame
iptables: 数据报文过滤
防火墙： 硬件/软件
规则： 匹配标准和处理办法
默认规则：
​ 关闭
匹配标准：
IP:源IP， 目标IP
TCP: SPORT, DPORT SYN=1,FIN=0,RST=0,ACK=0
UDP:SPORT, DPORT
ICMP: icmp-type
规则在内核空间
内核空间的TCPIP的协议栈上，开放给用户空间中的iptables API。
内核空间的工作框架：
用户空间的管理工具： system call
参考 openBSD
Linux 2.0：ipfw/firewall
Linux 2.2： ipchain/firewall Linux 2.4 ： iptables/netfilter
1：07
1/proc/sys/net/ipv4/ip_forward 路由决策发生在数据包到达网卡， 送到TCPIP协议栈上的那一刻。 然后先发生路由决策
netfilter 补充在tcp ip协议上的3个hook function。
多个规则，自上而下，逐个检查，
不做拒绝或者放行策略
4： 刚刚进入本机网卡，还没有到达路由表。（地址转换k&amp;rsquo;k）
5： 即将离开本机的时候，路由决策做出之后。
规则链
PREROUTING
INPUT
FORWARD
OUTPUT</description>
    </item>
    
    <item>
      <title></title>
      <link>https://airren.github.io/posts/cloudnative/Linux/jq/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Linux/jq/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://airren.github.io/posts/cloudnative/Linux/port/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Linux/port/</guid>
      <description>查看端口的占用情况
1netstat -atunlp 2 3# 用来查看系统当前网络状态信息， 包括端口、连接情况等 4# -t: TCP端口 5# -u: UDP端口 6# -l: 仅显示监听Socket LISTEN状态的Socket 7# -p: 显示进程标识符和程序名称，每一个socket都属于一个程序 8# -n: 不进行DNS解析 9# -a: 显示所有连接的端口 10 11 12 13 14lsof 15# 列出当前系统打开文件(list open files) 16# -i:[num] 指定端口 1nslookup # 域名解析 2host 3dig </description>
    </item>
    
    <item>
      <title></title>
      <link>https://airren.github.io/posts/cloudnative/Linux/route/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Linux/route/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://airren.github.io/posts/cloudnative/Linux/wan3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Linux/wan3/</guid>
      <description>mwan3
单线多拨</description>
    </item>
    
    <item>
      <title></title>
      <link>https://airren.github.io/posts/cloudnative/Linux/%E8%BF%9B%E7%A8%8B%E6%97%A5%E5%BF%97/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Linux/%E8%BF%9B%E7%A8%8B%E6%97%A5%E5%BF%97/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://airren.github.io/posts/cloudnative/Linux/%E8%BF%9B%E7%A8%8B%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Linux/%E8%BF%9B%E7%A8%8B%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/</guid>
      <description></description>
    </item>
    
    <item>
      <title>「Docker」 Docker 常用命令</title>
      <link>https://airren.github.io/posts/cloudnative/Docker/dock_tips/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Docker/dock_tips/</guid>
      <description>centos 安装docker 把yum包更新到最新
1yum update 安装需要的软件包
1yum install -y yum-utils device-mapper-persistent-data lvm2 设置yum源
1yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 查看所有仓库中所有docker版本，并选择特定版本安装
1yum list docker-ce --showduplicates | sort -r 2 3[root@MiWiFi-R3-srv ~]# yum list docker-ce --showduplicates | sort -r 4 * updates: mirrors.aliyun.com 5Loading mirror speeds from cached hostfile 6Loaded plugins: fastestmirror 7Installed Packages 8 * extras: mirrors.aliyun.com 9docker-ce.x86_64 3:18.09.0-3.el7 docker-ce-stable 10docker-ce.x86_64 18.06.1.ce-3.el7 docker-ce-stable 11docker-ce.x86_64 18.06.1.ce-3.el7 @docker-ce-stable 12docker-ce.x86_64 18.06.0.ce-3.el7 docker-ce-stable 13docker-ce.x86_64 18.03.1.ce-1.el7.centos docker-ce-stable 14docker-ce.x86_64 18.03.0.ce-1.el7.centos docker-ce-stable 15.</description>
    </item>
    
    <item>
      <title>「Docker」Docker 网络模式</title>
      <link>https://airren.github.io/posts/cloudnative/Docker/docker_net/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Docker/docker_net/</guid>
      <description>docker run创建容器时候，可以用-net指定容器的网络模式
1# host 模式 2--net=host 3# container模式 4--net=container:NameorId 5# none模式 6-net=none 7# bridge模式 8-net=bridge host 模式
如果启动容器的时候使用 host 模式，那么这个容器将不会获得一个独立的 Network Namespace，而是和宿主机共用一个 Network Namespace。容器将不会虚拟出自己的网卡，配置自己的 IP 等，而是使用宿主机的 IP 和端口。
例如，我们在 10.10.101.105/24 的机器上用 host 模式启动一个含有 web 应用的 Docker 容器，监听 tcp 80 端口。当我们在容器中执行任何类似 ifconfig 命令查看网络环境时，看到的都是宿主机上的信息。而外界访问容器中的应用，则直接使用 10.10.101.105:80 即可，不用任何 NAT 转换，就如直接跑在宿主机中一样。但是，容器的其他方面，如文件系统、进程列表等还是和宿主机隔离的。
container 模式 这个模式指定新创建的容器和已经存在的一个容器共享一个 Network Namespace，而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过 lo 网卡设备通信。
none模式 这个模式和前两个不同。在这种模式下，Docker 容器拥有自己的 Network Namespace，但是，并不为 Docker容器进行任何网络配置。也就是说，这个 Docker 容器没有网卡、IP、路由等信息。需要我们自己为 Docker 容器添加网卡、配置 IP 等。
bridge模式 bridge 模式是 Docker 默认的网络设置，此模式会为每一个容器分配 Network Namespace、设置 IP 等，并将一个主机上的 Docker 容器连接到一个虚拟网桥上。当 Docker server 启动时，会在主机上创建一个名为 docker0 的虚拟网桥，此主机上启动的 Docker 容器会连接到这个虚拟网桥上。虚拟网桥的工作方式和物理交换机类似，这样主机上的所有容器就通过交换机连在了一个二层网络中。接下来就要为容器分配 IP 了，Docker 会从 RFC1918 所定义的私有 IP 网段中，选择一个和宿主机不同的IP地址和子网分配给 docker0，连接到 docker0 的容器就从这个子网中选择一个未占用的 IP 使用。如一般 Docker 会使用 172.</description>
    </item>
    
    <item>
      <title>「K8s」Kubernates 安装</title>
      <link>https://airren.github.io/posts/cloudnative/Kubernates/KubernatesInstall/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Kubernates/KubernatesInstall/</guid>
      <description>环境准备 K8s 只能基于Linux环境部署，用Win/Mac的小伙伴们怎么在自己的PC上Setup环境呢。此时就推荐Canonical家的Multipass了，Canonical 是谁，当然是Ubuntu的母公司了。
安装 kubelet kubeadm kubectl 官方文档
1sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install -y apt-transport-https curl 2curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - 3cat &amp;lt;&amp;lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list 4deb https://apt.kubernetes.io/ kubernetes-xenial main 5EOF 6sudo apt-get update 7sudo apt-get install -y kubelet kubeadm kubectl 8sudo apt-mark hold kubelet kubeadm kubectl # 设置为不再更新 初始化 1swapoff -a 2kubeadm init ctl 3# kubeadm config images pull --v=10 4 # 国内正常网络不能从k8s.</description>
    </item>
    
    <item>
      <title>Docker Images</title>
      <link>https://airren.github.io/posts/cloudnative/Docker/docker_build_image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Docker/docker_build_image/</guid>
      <description>镜像仓库 Linux 本地镜像仓库：/var/lib/docker/image
镜像应该是分层存储的。
Docker images 是存储在镜像仓库服务Images Registry。Docker 客户端的镜像仓库服务是可配置的，默认使用的是Docker Hub。
镜像仓库服务包含多个镜像仓库 Image Repository（同一个镜像的不同版本）。一个镜像仓库中包含多个镜像Image。
Docker Hub 也分为Official Repository 和 Unofficial Repository。
1docker pull &amp;lt;repository&amp;gt;:&amp;lt;tag&amp;gt; 2// 如果省略tag默认会pull tag 为 latest的image。但是latest并不保证这是仓库中最新的镜像。 如果希望从第三方镜像服务仓库获取镜像(not Docker Hub)，则需要在镜像仓库名称前加上第三方镜像仓库服务的DNS名称。
1# gcr.io -&amp;gt; Google Container Images Registry. 2docker pull gcr.io/k8s-staging-nfd/node-feature-discovery:master Image Tag 不同的Images Tag可以绑定同一个Image ID
通过--filter 来过滤docker image ls 返回的内容
1docker image ls --filter dangling=true dangling image -&amp;gt; with out name &amp;amp; tag :.
通常因为构建新的镜像，为该镜像打了一个已经存在的标签。Docker会remove old image上的标签，将该标签标在新Image上。Old Image 就会变成 dangling image。</description>
    </item>
    
    <item>
      <title>Docker networking</title>
      <link>https://airren.github.io/posts/cloudnative/Docker/docker_networking/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Docker/docker_networking/</guid>
      <description>Docker networking is based on an open-source pluggable architecture called the Container Network Model(CNM). libnetwork is Docker&amp;rsquo;s real-work implementation of the CNM, adn it provides all of the Docker&amp;rsquo;s core networking capabilities. Drivers plug into libnework to provide specifice network topologies.
The theory At the highest level, Docker networking comprise three major components.
Single-host bridge network </description>
    </item>
    
    <item>
      <title>free命令</title>
      <link>https://airren.github.io/posts/cloudnative/Linux/free/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Linux/free/</guid>
      <description>type desc total physical mem used used = total- free-buffers-cache free Free/Unused memory shared It is here only for backward comopatibility buff/cache The combined memory used by the kernel buffers and page cache ans slabs. This memory can be reclaimed at any time if needed by the applicaitons. If you want buffers and cache to displayed in two separate columns, use -w options. available An estimate of the amount of memory that is availablew for starting new applications, without swaping.</description>
    </item>
    
    <item>
      <title>Git</title>
      <link>https://airren.github.io/posts/cloudnative/Linux/git/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Linux/git/</guid>
      <description>1# git cache user and passwd 2git config --global credential.helper cache </description>
    </item>
    
    <item>
      <title>K8s Cloud Native</title>
      <link>https://airren.github.io/posts/cloudnative/Kubernates/1-k8s-cloudnative/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Kubernates/1-k8s-cloudnative/</guid>
      <description>Kubenetes生产落地全程实践
核心概念
架构设计
认证授权
高可用集群搭建
二进制&amp;amp; kubeadm
3台master两台2 node
calico、coredns、dashboard
业务迁移到Kubenetes
Harbor
服务发现策略
IngressNginx
Docker 化服务、K8s、服务发现
CICD
namespace、resources、label
服务的调度与编排
健康检查
调度策略
部署策略
日志与监控
第2章 kubernetes快速入门 本章中将从核心概念、架构设计、认证授权以及集群搭建方案对比几方面，带领大家快速掌握kubernetes的重要知识点，助力快速入门。
2-1 了解kubernetes 舵手： 渔网、渔船 docker : 鲸鱼，集装箱 🐳
Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.
It groups containers that makeup application in to logical units for easy management and discovery.Kubernetes builds upon 15 years of experience of running production workloads at Google, combined with best-of-breed ideas and practices from the community.</description>
    </item>
    
    <item>
      <title>K8s ConfigMap</title>
      <link>https://airren.github.io/posts/cloudnative/Kubernates/9-k8s-configmap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Kubernates/9-k8s-configmap/</guid>
      <description>给Pod传递参数 通过环境变量给Pod传递参数 可以在container的描述文件中加入一个env的参数，值是一个数组，每个元素都是键值对，值是string。
1--- 2apiVersion: v1 3kind: Pod 4metadata: 5 name: first-pod 6 labels: 7 app: nginx 8spec: 9 containers: 10 - name: 00-simple-pod-nginx 11 image: nginx:1.17.0 12 env: 13 - name: INTERVAL 14 value: 30s 通过命令行参数给Pod传递参数 加入一个args配置，命令行参数是一个字符串数组</description>
    </item>
    
    <item>
      <title>K8s DaemonSet</title>
      <link>https://airren.github.io/posts/cloudnative/Kubernates/7-k8s-daemonset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Kubernates/7-k8s-daemonset/</guid>
      <description>NodeExporter 用于采集一个Host上的各种性能指标，并且暴露给Prometheus 采集。因为NodeExporter需要采集每一个Host的数据状态，所以就产生了一个需求，在每一个K8s的机器上都要运行一个Pod, 而实现这个需求的K8s资源类型就叫做DaemonSet。
DaemonSet DaemonSet的功能就是保证每个Node都运行Pod， 但是如果某个K8s的Node下线之后，对应的Pod也下线，还是保持每个Node一个Pod。如果新增一个Node，就会在新增的Node中增加一个Pod，保证所有的Node中有且只有一个当前Pod。
1--- 2apiVersion: apps/v1 3kind: DaemonSet 4metadata: 5 name: node-exporter 6 namespace: kube-system 7 labels: 8 k8s-app: node-exporter 9 kubernetes.io/cluster-service: &amp;#34;true&amp;#34; 10 addonmanager.kubernetes.io/mode: Reconcile 11 version: v1.2.2 12spec: 13 selector: 14 matchLabels: 15 k8s-app: node-exporter 16 version: v1.2.2 17 updateStrategy: 18 type: OnDelete 19 template: 20 metadata: 21 labels: 22 k8s-app: node-exporter 23 version: v1.2.2 24 spec: 25 priorityClassName: system-node-critical 26 containers: 27 - name: prometheus-node-exporter 28 image: &amp;#34;prom/node-exporter:v1.</description>
    </item>
    
    <item>
      <title>K8s Deployments</title>
      <link>https://airren.github.io/posts/cloudnative/Kubernates/5-k8s-deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Kubernates/5-k8s-deployment/</guid>
      <description>蓝绿部署 蓝绿部署是一种比较常见的部署方式，经常会用于新旧版本不兼容的情况。
先将新版本(绿版)的服务运行起来。 将所有的流量切到新版本的服务上 删除旧版本(蓝版)的服务 创建新版本的ReplicaSet，将Service指向新的Pods，删除旧版本的ReplicaSet
滚动升级 蓝绿部署可以有效保证业务的正确性，但是也带来了一定的风险，例如稳定性。
假设新部署的应用是有问题的，一旦切换之后就会导致业务的崩溃，造成损失。于是就有了稍微友好的升级方式，滚动升级。
先关闭一个旧版本的实例 开启一个新版本的实例用于替换旧版本 替换成功时候循环1和2，直到所有的实例升级完成。 在整个过程中，如果中途发现异常可以及时停手，及时止损。而且Kubernetes也在客户端中支持了这个特性。kubectl rolling-update。
升级前后RC的Selector都被改变了 操作都是在客户端执行的？ 金丝雀发布 金丝雀发布是滚动发布的一种特例，在滚动发布中，是不会等待的，除非中间出错了。但是有些时候，我们并不想要全都升级，可能只是处于POC的一些原因，我们只希望部分实例是新的，大部分是旧的，而这种情形，我们就称之为金丝雀发布。
升级少部分实例 查看效果，如果好，全部升级 如果不好，则不升级 声明式升级 前面介绍的这些升级发布方式在K8s上很多时候是半手工方式执行的，而Kubernetes作为一款DevOPS友好的系统，已经内置了对于部署方式的一种资源抽象，这个资源就是：Deployment。
Deployment &amp;ndash;&amp;gt; ReplicaSet &amp;ndash;&amp;gt; Pods
Deployment 存在的意义为：在升级应用程序时，需要引入额外的ReplicaSet，并协调新旧两个RS，使他们再根据彼此不断修改，而不会造成干扰。Deployment将这些运维过程都代码化，内置为自己的逻辑，从而让升级变得简单。
首先我们使用Deployment创建3个实例
1# deploy.yaml 2--- 3apiVersion: apps/v1 4kind: Deployment 5metadata: 6 name: first-deployment 7 labels: 8 app: simple-pod-deployment 9spec: 10 replicas: 3 11 selector: 12 matchLabels: 13 app: simple-pod-deployment 14 template: 15 metadata: 16 name: simple-pod-deployment 17 labels: 18 app: simple-pod-deployment 19 spec: 20 containers: 21 - name: simple-pod-de 22 image: lukelau/rest-docker:0.</description>
    </item>
    
    <item>
      <title>K8s Job</title>
      <link>https://airren.github.io/posts/cloudnative/Kubernates/6-k8s-job/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Kubernates/6-k8s-job/</guid>
      <description>Replicas 和deployment这两类资源都是用于控制workload的。这两种类型的资源一般都是持续运行的，同时还有一些辅助方式帮助workload出现异常时恢复，以及根据情况进行动态伸缩的特性。
Job 根据Job的定义创建出对应的Pod，然后关注Pod的状态， 直到满足定义。例如Pod执行成功了或者执行失败了，并且达到了重试次数。
1--- 2apiVersion: batch/v1 3kind: Job 4metadata: 5 name: pi 6spec: 7 template: 8 spec: 9 containers: 10 - name: pi 11 image: perl 12 command: [&amp;#34;perl&amp;#34;, &amp;#34;-Mbignum=bpi&amp;#34;,&amp;#34;-wle&amp;#34;, &amp;#34;print bpi(2000)&amp;#34;] 13 restartPolicy: Never 14 backoffLimit: 4 Job正常执行结束后结果如上图。这是一个只执行一次的Job。它的操作方式就是创建一个Pod，然后运行一遍，然后就退出。如果想执行多次，则只需要增加一个参数
1completions: 2 执行2次时创建了两个Pod，然后保证这两个Pod都执行成功。
我们在使用Deployment等Workload的时候，一般会指定restartPolicy，默认都是RestartOnFail。在Job中不能这么指定，因为这个逻辑应该由Job来控制， 而不是让Pod来控制。
CronJob 定时任务， CronJob就是在Job的基础上加上了周期定义的API
1--- 2apiVersion: batch/v1 3kind: CronJob 4metadata: 5 name: batch-job-pi 6spec: 7 schedule: &amp;#34;0,15,30,45 * * * *&amp;#34; 8 jobTemplate: 9 spec: 10 template: 11 metadata: 12 labels: 13 app: pi-job 14 spec: 15 containers: 16 - name: pi 17 image: perl 18 command: [&amp;#34;perl&amp;#34;, &amp;#34;-Mbignum=bpi&amp;#34;,&amp;#34;-wle&amp;#34;, &amp;#34;print bpi(2000)&amp;#34;] 19 restartPolicy: Never </description>
    </item>
    
    <item>
      <title>K8s Overview</title>
      <link>https://airren.github.io/posts/cloudnative/Kubernates/1-k8s-overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Kubernates/1-k8s-overview/</guid>
      <description>Node 节点主要负责容器的管理，用于运行容器和保证容器的状态。默认情况下Master节点不承担Node节点的功能，但是可以通过特殊的配置让Master节点也可作为Node节点。
Etcd用于存储Kubernetes的元数据，但是不要求一定要以容器的形式运行。
1kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=192.168.0.22 --pod-network-cidr pod 的ippool, --apiserver-advertise-address 为暴露的k8sAPI调用IP.
节点(Node)
一个Node是一个运行K8s的主机，作为K8s worker, 通常称之为Minion。每个节点都运行如下K8s关键组件：
kubelet：主节点代理 kube-proxy: service使用其将链接路由到Pod docker/rocket: k8s 使用容器技术创建容器 容器组(Pod)
一个Pod对应若干容器组成的一个容器组，同一个Pod内的容器共享一个存储卷(volume)，同一个Pod中的容器共享一个网络Namespace,可以使用localhost互相通信。
Pod是短暂的，不是持续性实体。一个Pod是一个特定的应用打包集合，包含一个或者多个容器。和运行的容器类似，一个Pod只有很短的运行周期。Pod被调度到一个Node运行，直到容器的生命周期结束或者其被删除。
容器组生命周期(Pod Status)
包含所有容器状态集合，包括容器组状态类型，容器组生命周期，事件，重启策略，以及replication controllers.
标签(labels)
标签是用来连接一组对象的，比如容器组Pod。lable可以用来组织和选择子对象。 一个Label是attach到Pod的一对键值对，用来传递用户定义的属性。
Replication Controllers
主要负责指定数量的Pod在同一时间一起运行。Replication controller 确保任意时间都有指定数量的Pod副本在运行。如果为某个Pod创建了Replication Controller并且指定为3副本，它会创建3个Pod,并持续监控他们。如果某个Pod不响应，那么Replication controller 会替换它，保持Pod总数为3.
当创建Replication Controller时，需要指定两个东西。
Pod模板： 用来创建Pod副本的模板 Label：Replication Controller 需要监控的Pod的Label 现在已经创建了Pod的一些副本，那么在这些副本上如何负载均衡呢，我们需要的是service
Service：
如果Pod是短暂的，那么重启时IP地址可能会变，怎么才能从前端容器正确可靠的指向后台容器呢。
Service是定义一系列Pod以及访问这些Pod的策略的一层抽象。Service通过Label找到Pod组。因为service是抽象的，所在在图表里通常看不到他们的存在。
现在假定有两个后台Pod,并且定义后台service名称为“backend-service”，label选择器为(tier=backend,app=myapp)。backend-service的Service会完成如下两件重要的事情：
会为Service创建一个本地集群的DNS入口，因此前端只需要DNS查找主机名为“backend-service”就能够解析出前端应用程序可用的IP地址 现在前端已经得到了后台服务的IP地址，但是它应该访问2个后台Pod中的哪一个呢。Service在这两个后台Pod之间提供透明的负载均衡，会将请求发给其中的任意一个。通过每个Node上运行的代理 kube-proxy完成。 Kubernetes Master
集群拥有一个K8s Master,K8s Master 提供集群的独特视角，并拥有一系列组件，如Kubernetes API server. API server 提供可以用来和集群交互的REST 端点。Master 节点包含用来创建和复制Pod的Replication Controller.
参考资料：</description>
    </item>
    
    <item>
      <title>K8s StafulSet</title>
      <link>https://airren.github.io/posts/cloudnative/Kubernates/8-k8s-stafulset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Kubernates/8-k8s-stafulset/</guid>
      <description>前面介绍的几种workload都有一个共性，那就是创建出来的Pod都是一致的。所谓的一致就是说，假设我们使用的是ReplicaSet，创建了3个Pod，那么这3个Pod创了名字一定不一样之外，其他属性可能都是一样的，包括运行时的参数和模式以及数据存储。
如果是Web Service，数据保存到后端的DB中，上述逻辑是没有问题的。
如果使用ReplicaSet来部署一个DB的多实例， 就可肯能存在问题了。
数据持久化，一般使用PVC，当使用PVC 和PV的时候
​
ReplicaSet -&amp;gt; Pods(3 个) -&amp;gt; 持久卷声明 -&amp;gt; 持久卷
3个Pod的数据都写到同一个Pv中，这样肯定是不行的？？？
StatefulSet 为了解决Pod的状态性的问题，K8s引入了StatefulSet的概念
Pod有单独的存储和固定的网络标识 需要配备一个headless Service，用于DNS 可以通过DNS快速发现其他的Pod 可以直接通过Pod DNS通信 每个Pod都可以通过DNS访问到，这种特性在其他workload中是不能实现的。通过StatefulSet可以让Pod持有状态，即使因为故障Pod重建了，那么对应的Pod的名字和数据都会保留，和重建之前没有什么区别。
使用StatefulSet 必须建立一个HeadlessService，然后绑定这个headlessservice到StatefulSet。
以MongoDB为例，创建一个StatefulSet，因为还没有介绍到PVC和PV的内容，所以MongoDB将使用本地存储卷。
创建Headless Service
创建 StatefulSet
通过DNS访问Pod
在StatefulSet的Pod中可以通过DNS直接访问其他Pod。
1mongo --host mongo-1.mongo 可以通过&amp;lt;pod-name&amp;gt;.&amp;lt;service-name&amp;gt;的形式访问Pod。这其实和StatefulSet的设计是有关系的，在类似的Deployment中Pod的名字是不固定的，而在StatefulSet中，Pod的名字是固定的。
和ReplicaSet对比
因为有状态的Pod彼此不同，通常希望操作的是其中的特定的一个，所以StatefulSet通常要求你创建一个用来记录每个Pod网络标记的HeadlessService。通过这个Service，每个Pod都拥有独立的DNS记录，而这在ReplicaSet中是不行的？（如果为ReplicaSet创建一个Headless Service会发生啥？） 因为StatefulSet缩容任何时候只会操作一个Pod实例，所以有状态应用的缩容不会很迅速。 StatefulSet在有实例不健康的情况下，是不允许缩容的。 持久存储 一个StatefulSet可以拥有一个或者多个卷声明模板，这些声明会在创建Pod前创建出来，绑定到一个Pod的实例上。 扩容StatefulSet会创建两个API对象，一个Pod和一个卷声明；但是缩容StatefulSet却会删除一个Pod对象，而会留下PVC，因为一旦删除PVC则意味着PV会被回收。 StatefulSet at-most-one
Kubernetes 必须保证两个拥有相同标记和绑定相同持久卷声明的有状态的Pod实例不会同时运行。一个StatefulSet必须保证有状态的实例的 at-most-one 语义。也就是说StatefulSet必须保证一个Pod不再运行后，才会去创建它的替换Pod。</description>
    </item>
    
    <item>
      <title>Kubenetes Pod</title>
      <link>https://airren.github.io/posts/cloudnative/Kubernates/2-k8s-pod/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Kubernates/2-k8s-pod/</guid>
      <description>Pod
在Kubernetes中，一切都是资源，你可以通过create/get/describe/delete 来操作这些资源。
在操作一种资源之前，我们需要先对这个资源进行定义，在k8s中常用的是yaml配置文件配置。
1# 00-siample-pod.yaml 2--- 3apiVersion: v1 4kind: Pod 5metadata: 6 name: first-pod 7 labels: 8 app: nginx 9spec: 10 containers: 11 - name: 00-simple-pod-nginx 12 images: nginx:1.17.0 apiVersion：资源的版本，可以理解为你要创建的是 PodV1{}还是PodVn{}
kind: 资源的类型
metadata：
​	name： 创建出来的资源的名字
​ labels：与其他资源粒度或者操作的关联
spec: 资源的参数
通过kubectl apply -f创建资源
1kubectl apply -f 00-simple-pod.yaml 如果需要更新资源，修改yaml后，重新kubectl apply -f xxx.yaml 就可以。
获取Pod状态
1kubectl get pod first-pod 2# get more detail 3kubectl get pod first-pod -o wide 4# get all pods of all-namespace 5kubectl get po -A 获取Pod 详情</description>
    </item>
    
    <item>
      <title>Kubernetes CRD</title>
      <link>https://airren.github.io/posts/cloudnative/Kubernates/10-k8s-crd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Kubernates/10-k8s-crd/</guid>
      <description>Kubernetes 1.16 正式GA了CRD。
CRD介绍 声明式编程 在Kubernetes中我们使用了Deployment/DamenSet/tatefulSet来管理应用workdload，使用Service/Ingress来管理应用的
https://zhuanlan.zhihu.com/p/34445114</description>
    </item>
    
    <item>
      <title>Kubernetes Replication Controller</title>
      <link>https://airren.github.io/posts/cloudnative/Kubernates/4-k8s-replic/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Kubernates/4-k8s-replic/</guid>
      <description>Replication Controller 1# replica.yaml 2--- 3apiVersion: v1 4kind: ReplicationController 5metadata: 6 name: first-replic 7spec: 8 replicas: 3 9 template: 10 metadata: 11 name: simple-pod 12 labels: 13 app: simple-pod 14 spec: 15 containers: 16 - name: timemachine 17 image: lukelau/rest-docker:0.0.1 18 args: 19 - -server.addr=0.0.0.0:8000 以上配置会保证Pod的数量稳定为3个。当我们删除一个Pod之后，Replication Controller就会创建出一个新的Pod来维持Pod的数量。
RC之所以会发现Pod已经挂掉了，是因为探针(Container probes)的存在。在K8s中， kubelet会通过指定的探针方式去探测容器是否存活。
三种探针方式
三种类型的handler
ExecAction： Executes a specified sommand inside the container. The diagnostic is considered successful if the command exits with a status code of 0.</description>
    </item>
    
    <item>
      <title>Kubernetes Service</title>
      <link>https://airren.github.io/posts/cloudnative/Kubernates/3-k8s-service/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Kubernates/3-k8s-service/</guid>
      <description>Pod是最小的单元， 往往我们在运行一个应用的时候，对Pod有一些额外的要求，例如高可用或者多实例，这就意味着如果你只记住一个Pod的IP，那么很多时候时有问题的。 例如高可用，可能会因为Pod的重建而改变，这样你记住的那个IP就失效了。一个很自然的想法就是固定IP。
为什么要使用service
因为K8s里面的Pod是可以被调度的，并且重建的，所以没有固定的IP Pod的数量可能不只一个，当有多个Pod实例的时候负载均衡的需求。 1# service.yaml 2--- 3apiVersion: v1 4kind: Service 5metadata: 6 name: first-service 7spec: 8 type: NodePort 9 selector: 10 app: nginx 11 ports: 12 - protocol: TCP 13 port: 5580 14 targetPort: 80 15 nodePort: 32280 selector 过滤携带label app=nginx的pod
targetPort： pod 提供服务的端口
service代理Pod的内部端口为5580： 内部访问，固定IP：Port
service代理Pod的外部端口为32280， 通过k8s集群的IP，可以进行 外部可访问
1# create service 2kubectl apply -f service.yaml 3# get service status 4kubectl get service first-service 外部访问service
上面这个配置之所以可以在外部访问，是因为制定了Service的Type为NodePort， 在K8s中如果不指定这个Type的话，service时只能在K8s集群内部访问的，集群外部是访问不了的。</description>
    </item>
    
    <item>
      <title>参数传递</title>
      <link>https://airren.github.io/posts/cloudnative/Linux/%E5%8F%82%E6%95%B0%E4%BC%A0%E9%80%92/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Linux/%E5%8F%82%E6%95%B0%E4%BC%A0%E9%80%92/</guid>
      <description>1. 顺序传参 参数名 作用 $0 filename $1/$2/$3 &amp;hellip;.$n 第n个参数 2. OPT 只能用单个字母
1#!/bin/bash 2 3while getopts &amp;#34;:n:a:h&amp;#34; optname; do 4	case &amp;#34;$optname&amp;#34; in 5	&amp;#34;n&amp;#34;) 6	echo &amp;#34;get option -name, value is $OPTARG&amp;#34; 7	;; 8	&amp;#34;a&amp;#34;) 9	echo &amp;#34;get option -age, value is $OPTARG&amp;#34; 10	;; 11	&amp;#34;h&amp;#34;) 12	echo &amp;#39; 13	-n name of user 14	-a age of user 15	&amp;#39; 16	;; 17	&amp;#34;:&amp;#34;) 18	echo &amp;#34;No argument value for option $OPTARG&amp;#34; 19	;; 20K	&amp;#34;?</description>
    </item>
    
    <item>
      <title>容器技术基础</title>
      <link>https://airren.github.io/posts/cloudnative/Kubernates/Container-Intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Kubernates/Container-Intro/</guid>
      <description>为什么容器里只能跑“一个进程”？ 为什么我原先一直在用某个JVM参数，在容器里就不好使了？ 为什么kubernetes就不能固定IP地址？容器网络联不通又该如何去debug？ Kubernetes中的StatefulSet和Operator到底什么区别？PC和PVC这些概念又该怎么用？ Linux进程模型对容器本身的重要意义，控制器模式对整个K8s项目提纲挈领的作用？ 从PaaS到K8s PaaS PaaS(Platform as a Service) 应用托管。
Docker镜像，其实就是一个压缩包，直接由一个完整的操作系统的所有文件和目录构成。
其实只打包了文件系统，不包括操作系统的内核。各种内核相关的模块或者特性支持完全依赖于宿主机。
通过docker build 打包镜像，docker run 运行镜像，docker run创建的沙盒，就是使用Linux Cgroups和Namespace机制创建出来的隔离环境。解决了应用打包这个根本性问题。
Swarm swarm 提供集群管理功能
单机docker项目
1docker run &amp;lt;container-name&amp;gt; 多机docker项目
1docker run -H &amp;#39;swarm cluster API&amp;#39; &amp;lt;container-name&amp;gt;	Fig 项目 Fig项目第一次在开发者面前提出了容器编排(Container Orchestration)的概念。
加入用户现在需要部署的是应用容器A、数据库容器B、负载均衡容器C，那么Fig就允许用户把ABC三个容器定义在一个配置文件中， 并且可以指定他们之间的关联关系，比如容器A需要访问数据库B。定义好之后，只需要执行一条非常简单的指令。
1fig up Fig就会把这些容器的定义和配置交给DockerAPI按照访问逻辑一次创建。而容器A和B之间的关联关系，也会交给docker的Link功能通过写入hosts文件的方式进行配置。更重要的是，你还可以在Fig的配置文件里定义各种容器的副本个数等编排参数。
Fig 项目被Docker收购后更名为Compose。
Libcontainer LibContainer -&amp;gt; RunC
以RunC为依据，制定容器和镜像的标准和规范。
OCI(Open Container Initiative), 意在将容器运行时和镜像的实现从Docker项目中完全剥离出来。
Containerd 容器运行时
进程隔离与限制 程序被执行起来，它就从磁盘上的二进制文件，变成了计算机 内存中的数据，寄存器里的值，堆栈中的指令、被打开的文件，以及各种设备的状态信息的一个集合。像这样一个程序运行起来后的计算机执行环境的总和，就是：进程。
对于进程来说，它的静态表现就是程序，一个二进制文件；而一旦运行起来，就变成了计算机数据和状态的总和，这就是进程的动态表现。
容器技术的核心功能，就是通过约束和修改进程的动态表现，从而为其创造出一个“边界”。对于大多数Linux容器来说， Cgroups是用来制造约束的主要手段， 而Namespace技术则是用来修改进程视图的主要方法。
隔离 1docker run -it busybox /bin/sh What is BusyBox?</description>
    </item>
    
    <item>
      <title>策略路由</title>
      <link>https://airren.github.io/posts/cloudnative/Linux/%E7%AD%96%E7%95%A5%E8%B7%AF%E7%94%B1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://airren.github.io/posts/cloudnative/Linux/%E7%AD%96%E7%95%A5%E8%B7%AF%E7%94%B1/</guid>
      <description>策略路由 所有来自网络A的包选择X路径，其他选择Y路径。或者说所有TOS为A的包选择路径F,其他选择路径K.
多表路由（multiple Routing Tables） 传统的路由算法仅仅使用一张表，有些情形下是需要使用多路由表的。
规则（Rule） 所有来自192.168.1.5的包，使用路由表10,本规则的优先级是990
所到到192.168.127.117的包使用路由表11, 本规则的优先级是991；
规则三要素
什么样的包应用本规则 ip route add 增加路由到指定的路由表，默认为main表</description>
    </item>
    
    
  </channel>
</rss>
